{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04812e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f627ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.gutenberg.org/files/98/98-0.txt')\n",
    "data = response.text.split('\\n')\n",
    "data = data[108:]\n",
    "data = \" \".join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "544d3b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(doc):\n",
    "  tokens = doc.split()\n",
    "  table = str.maketrans('', '', string.punctuation)\n",
    "  tokens = [w.translate(table) for w in tokens]\n",
    "  tokens = [word for word in tokens if word.isalpha()]\n",
    "  tokens = [word.lower() for word in tokens]\n",
    "  return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4159aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = clean_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d675f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 5+1\n",
    "text_sequences = []\n",
    "for i in range(train_len,len(tokens)):\n",
    "    seq = tokens[i-train_len:i]\n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3283c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = {}\n",
    "count = 1\n",
    "for i in range(len(tokens)):\n",
    "    if tokens[i] not in sequences:\n",
    "        sequences[tokens[i]] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4ec4f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "775d399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)+1\n",
    "\n",
    "n_sequences = np.empty([len(sequences),train_len], dtype='int32')\n",
    "for i in range(len(sequences)):\n",
    "    n_sequences[i] = sequences[i]\n",
    "\n",
    "train_inputs = n_sequences[:,:-1]\n",
    "train_targets = n_sequences[:,-1]\n",
    "train_targets = to_categorical(train_targets, num_classes=vocabulary_size)\n",
    "seq_len = train_inputs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3390c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(train_inputs, train_targets):\n",
    "    true_positives = K.sum(K.round(K.clip(train_inputs * train_targets, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(train_inputs, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(train_inputs, train_targets):\n",
    "    true_positives = K.sum(K.round(K.clip(train_inputs * train_targets, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(train_targets, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(train_inputs, train_targets):\n",
    "    precision = precision_m(train_inputs, train_targets)\n",
    "    recall = recall_m(train_inputs, train_targets)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d91e160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies={\n",
    "    'f1_m':f1_m,\n",
    "    'precision_m':precision_m,\n",
    "    'recall_m':recall_m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "515fad07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My friend is dealing with\n",
      "[30, 228, 23, 13] [[  0  30 228  23  13]]\n",
      "Next word suggestion: your\n",
      "Next word suggestion: this\n",
      "Next word suggestion: my\n"
     ]
    }
   ],
   "source": [
    "model=keras.models.load_model(\"LSTM.h5\",custom_objects=dependencies)\n",
    "input_text = input().strip().lower()\n",
    "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "print(encoded_text, pad_encoded)\n",
    "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\n",
    "  pred_word = tokenizer.index_word[i]\n",
    "  print(\"Next word suggestion:\",pred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c4c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
